{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "## Lab 11: Basic Clustering\n",
    "### Date: April 20th, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this lab is to practice implementing some basic clustering algorithms in sci-kit learn.  Specifically we will start with PCA, t-SNE and then move on to MDS and K-means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary packages from sci-kit learn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.cluster import vq # Specifically uesful for K-means clustering\n",
    "from sklearn import cluster  # Clustering algorithms such as K-means and agglomerative\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import MiniBatchKMeans \n",
    "from sklearn.metrics.pairwise import pairwise_distances_argmin\n",
    "from sklearn.manifold import MDS #Import the multidimensional scaling module\n",
    "from scipy.spatial.distance import squareform #Import squareform, which creates a symmetric matrix from a vector\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = datasets.load_digits(as_frame=True)\n",
    "X,y = df.data, df.target\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2, whiten=True)\n",
    "pca.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = pca.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = range(len(df.target_names))\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i, c, label in zip(target_ids, 'rgbcmykw', df.target_names):\n",
    "    plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], c=c, label=label)\n",
    "plt.xlabel('PC1')\n",
    "plt.ylabel('PC2')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## T-distributed Stochastic Neighbor Embedding (t-SNE) using scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A statistical method for visualizing high-dimensional data by giving each datapoint a location in either a two or three-dimensional space which can be easily visualized using a scatter plot. It is not recommended for use in analysis such as clustering or outlier detection since it does not necessarily preserve densities or distances well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_embedded = TSNE(n_components=2).fit_transform(X)\n",
    "X_embedded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = range(len(df.target_names))\n",
    "plt.figure(figsize=(5, 5))\n",
    "for i, c, label in zip(target_ids, 'rgbcmykw', df.target_names):\n",
    "    plt.scatter(X_embedded[y == i, 0], X_embedded[y == i, 1], c=c, label=label)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Dimensional Scaling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load a custom dataset based on academic disciplines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.genfromtxt('fieldsMDS.csv',delimiter=',') # load file as data\n",
    "dataVec = np.ndarray.flatten(data) #Turning the matrix into a vector of pairwise distances\n",
    "dataVec = dataVec[~np.isnan(dataVec)] #Remove all nans to leave only pairwise distances remaining\n",
    "D = squareform(dataVec) #Create the distance matrix\n",
    "fieldNames = ['Math', 'Physics', 'Chemistry', 'Biology', 'Psych', 'Neuro', 'Econ', 'Sociology', 'DS', 'CS']; #These are the academic fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mds = MDS(n_components=2, n_init=100, max_iter = 10000, dissimilarity='precomputed') #Create the mds object\n",
    "#Components: Looking for a 2D solution, n_init: Number of runs with random initial starting positions, max_iter: Max number of iterations per run, dissimilarity: We already did it, from out distance matrix, \n",
    "mdsSolution = mds.fit_transform(D) #Actually run the mds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(mdsSolution[:,0], mdsSolution[:,1], color='blue') #Making the plot, first 2 dimensions\n",
    "for ii in range(len(mdsSolution)):\n",
    "    plt.text(mdsSolution[ii,0], mdsSolution[ii,1],fieldNames[ii])\n",
    "\n",
    "plt.xlabel('MDS axis 1')\n",
    "plt.ylabel('MDS axis 2')\n",
    "plt.show()\n",
    "\n",
    "print(mds.stress_) #How low could we get the stress? [Un-normalized]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "\n",
    "We'll start by looking at the sci-kit learn implementation of K-means for a synthetic dataset that has distinct clusters.  The cell below generates a synthetic dataset with 4 well-separated clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the synthetic dataset.\n",
    "X1 = np.random.multivariate_normal(size = 10, mean = np.array([3, 3]), cov = np.identity(2))\n",
    "X2 = np.random.multivariate_normal(size = 10, mean = np.array([-3, 3]), cov = np.identity(2))\n",
    "X3 = np.random.multivariate_normal(size = 10, mean = np.array([-3, -3]), cov = np.identity(2))\n",
    "X4 = np.random.multivariate_normal(size = 10, mean = np.array([3, -3]), cov = np.identity(2))\n",
    "\n",
    "X = np.vstack([X1,X2,X3,X4])\n",
    "plt.scatter(X1[:,0], X1[:,1], c = 'b', marker = 's', label = 'Cluster 1')\n",
    "plt.scatter(X2[:,0], X2[:,1], c = 'r', marker = 'o', label = 'Cluster 2')\n",
    "plt.scatter(X3[:,0], X3[:,1], c = 'g', marker = 'p', label = 'Cluster 3')\n",
    "plt.scatter(X4[:,0], X4[:,1], c = 'm', marker = '+', label = 'Cluster 4')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title(r'Synthetic dataset with 4 clusters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distortion\n",
    "\n",
    "A key quantity in helping determine a good number of clusters to use is the distortion\n",
    "$$\n",
    "J = \\sum_{i=1}^N \\|{\\bf x}_i - \\mu_{C(i)}\\|^2\n",
    "$$\n",
    "where ${\\bf x}_i$ are the data points, $C(i) \\in \\{1,\\ldots,K\\}$ is the cluster assignment for ${\\bf x}_i$ and $\\mu_j$ for $j=1,\\ldots,K$ are the centers of the clusters.  Intuitively, the distortion captures the unexplained variation in the dataset after accounting for the clusters.  If $K = N$, then $\\mu_{C(i)} = {\\bf x}_i$ and the distortion will be 0.  In this case there is a cluster at every data point so intuitively there is no unexplained variation.  However, having a large number of clusters is often not very useful since we will likely be overfitting to noise in the data.  There will often be a certain point where the distortion starts to decrease more slowly.  This is called the \"elbow method\", which is what we plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to keep track of the distortions for K=1,...,N.\n",
    "distortions = np.zeros(len(X))\n",
    "\n",
    "for k in range(1, len(X) + 1):\n",
    "    kmeans = cluster.KMeans(k) # K-means object in sci-kit learn with k clusters.\n",
    "    kmeans.fit(X)              # This is the line that actually runs the K-means algorithm.\n",
    "    distortions[k-1] = kmeans.inertia_ # In sci-kit learn the distortion is called the inertia.\n",
    "\n",
    "# Plot the results.\n",
    "plt.plot(np.arange(1, len(X)+1, 1), distortions, 'b-x', lw = 2)\n",
    "plt.xlabel(r'Clusters $k$')\n",
    "plt.ylabel(r'Distortion')\n",
    "plt.title(r'Elbow Curve for K-means')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed we see from the plot above that the distortion decreases rapidly up until $k=4$, which was the true number of clusters for our data.  After this point we begin overfitting to the noise and the distortion will not decrease as much.  A general heuristic is to choose $k$ where the kink, or elbow, in the curve occurs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This data was well-separated, however.  Let's see how the distortion behave whenever there is overlap between the clusters.  The cell below generates the new fake dataset with the clusters closer together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the new synthetic dataset.  The only difference from before is that the means are closer together now.\n",
    "X1 = np.random.multivariate_normal(size = 10, mean = np.array([1, 1]), cov = np.identity(2))\n",
    "X2 = np.random.multivariate_normal(size = 10, mean = np.array([-1, 1]), cov = np.identity(2))\n",
    "X3 = np.random.multivariate_normal(size = 10, mean = np.array([-1, -1]), cov = np.identity(2))\n",
    "X4 = np.random.multivariate_normal(size = 10, mean = np.array([1, -1]), cov = np.identity(2))\n",
    "\n",
    "X = np.vstack([X1,X2,X3,X4])\n",
    "plt.scatter(X1[:,0], X1[:,1], c = 'b', marker = 's', label = 'Cluster 1')\n",
    "plt.scatter(X2[:,0], X2[:,1], c = 'r', marker = 'o', label = 'Cluster 2')\n",
    "plt.scatter(X3[:,0], X3[:,1], c = 'g', marker = 'p', label = 'Cluster 3')\n",
    "plt.scatter(X4[:,0], X4[:,1], c = 'm', marker = '+', label = 'Cluster 4')\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title(r'Synthetic dataset with 4 clusters')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of the distortions for k=1,...,N.\n",
    "distortions = np.zeros(len(X))\n",
    "\n",
    "# Same code as before.\n",
    "for k in range(1, len(X) + 1):\n",
    "    kmeans = cluster.KMeans(k)\n",
    "    kmeans.fit(X)\n",
    "    distortions[k-1] = kmeans.inertia_\n",
    "\n",
    "# Plot the results.\n",
    "plt.plot(np.arange(1, len(X)+1, 1), distortions, 'b-x', lw = 2)\n",
    "plt.xlabel(r'Clusters $k$')\n",
    "plt.ylabel(r'Distortion')\n",
    "plt.title(r'Elbow Curve for K-means')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now there is a more gradual decrease in the distortion and it is not as clear what choice of $k$ one should use.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing the Cluster Centers \n",
    "One feature of K-means is that it is prone to becoming stuck in local minimum and is therefore sensitive to the initial cluster centers that are chosen.  The scipy implementation `kmeans2` allows for more flexibility in choosing the initial conditions so we also show it here as an alternative to sci-kit's implementation.  Now we see how the distortion changes after each iteration in K-means for different initializations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the distortion vs iteration for three different initial means.\n",
    "\n",
    "# Helper function to compute the distortion using the data X and the computed cluster centers\n",
    "# and labels for each point.\n",
    "def distortion(X, centers, labels):\n",
    "    N = X.shape[0]\n",
    "    J = 0\n",
    "    for i in range(N):\n",
    "        J += np.linalg.norm(X[i] - centers[labels[i]])**2\n",
    "    return J\n",
    "\n",
    "# Only use 10 iterations of K-means.\n",
    "max_iter = 10\n",
    "distortions_1 = np.zeros(max_iter)\n",
    "distortions_2 = np.zeros(max_iter)\n",
    "distortions_3 = np.zeros(max_iter)\n",
    "\n",
    "# 3 different initializations.\n",
    "K = 4 # 4 clusters\n",
    "np.random.seed(325) # Random seed is only chosen to emphasize difference between initializations.\n",
    "                    # This line could be removed.\n",
    "centers1, labels1 = vq.kmeans2(data = X, k = K, iter = 1, minit = '++')      # k-means++ initialization\n",
    "centers2, labels2 = vq.kmeans2(data = X, k = K, iter = 1, minit = 'random')  # points sampled from a Gaussian\n",
    "centers3, labels3 = vq.kmeans2(data = X, k = K, iter = 1, minit = 'points')  # points chosen from the dataset\n",
    "\n",
    "distortions_1[0] = distortion(X, centers1, labels1)\n",
    "distortions_2[0] = distortion(X, centers2, labels2)\n",
    "distortions_3[0] = distortion(X, centers3, labels3)\n",
    "\n",
    "for i in range(1, max_iter):\n",
    "    # Do 1 iteration of K-means using the cluster centers from the last iteration.\n",
    "    centers1, labels1 = vq.kmeans2(data = X, k = centers1, iter = 1, minit = 'matrix')\n",
    "    centers2, labels2 = vq.kmeans2(data = X, k = centers2, iter = 1, minit = 'matrix')\n",
    "    centers3, labels3 = vq.kmeans2(data = X, k = centers3, iter = 1, minit = 'matrix')\n",
    "    \n",
    "    distortions_1[i] = distortion(X, centers1, labels1)\n",
    "    distortions_2[i] = distortion(X, centers2, labels2)\n",
    "    distortions_3[i] = distortion(X, centers3, labels3)\n",
    "\n",
    "# Plot the results\n",
    "plt.plot(np.arange(1, max_iter + 1), distortions_1, 'r-s', label = '++')\n",
    "plt.plot(np.arange(1, max_iter + 1), distortions_2, 'b-o', label = 'random')\n",
    "plt.plot(np.arange(1, max_iter + 1), distortions_3, 'm-x', label = 'points')\n",
    "plt.xlabel(r'Iteration')\n",
    "plt.ylabel(r'Distortion')\n",
    "plt.title(r'Convergence of K-means for different initializations')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that all three curves have leveled-off indicates the K-means has converged.  However, the distortion is different meaning we have congevered to different local optima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MiniBatch KMeans\n",
    "Given: `k`, mini-batch size `b`, iterations `t`, data set `X`\n",
    "Initialize each $c \\in C$ with an `x` picked randomly from `X`\n",
    "```python\n",
    "v ← 0\n",
    "for i = 1 to t do\n",
    "      M ← b examples picked randomly from X\n",
    "      for x ∈ M do\n",
    "            d[x] ← f (C, x)         // Cache the center nearest to x\n",
    "      end for\n",
    "      for x ∈ M do\n",
    "            c ← d[x]                // Get cached center for this x\n",
    "            v[c] ← v[c] + 1         // Update per-center counts\n",
    "            η ← 1 / v[c]            // Get per-center learning rate\n",
    "            c ← (1 − η)c + ηx       // Take gradient step\n",
    "      end for\n",
    "end for\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate sample data\n",
    "# Try 3e+3, 1e+4, 1e+5, 1e+6, 1e+7\n",
    "np.random.seed(0)\n",
    "\n",
    "batch_size = 45\n",
    "centers = [[1, 1], [-1, -1], [1, -1]]\n",
    "n_clusters = len(centers)\n",
    "X, labels_true = datasets.make_blobs(n_samples=3000, centers=centers, cluster_std=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clustering with Means\n",
    "\n",
    "k_means = KMeans(init='k-means++', n_clusters=3, n_init=10)\n",
    "t0 = time.time()\n",
    "k_means.fit(X)\n",
    "t_batch = time.time() - t0\n",
    "print(\"Time KMeans: \", t_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clustering with MiniBatchKMeans\n",
    "\n",
    "mbk = MiniBatchKMeans(init='k-means++', n_clusters=3, batch_size=batch_size,\n",
    "                      n_init=10, max_no_improvement=10, verbose=0)\n",
    "t0 = time.time()\n",
    "mbk.fit(X)\n",
    "t_mini_batch = time.time() - t0\n",
    "print(t_mini_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets see whether the two algorithms lead to similar cluster assignments or not. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot result\n",
    "\n",
    "fig = plt.figure(figsize=(8, 3))\n",
    "fig.subplots_adjust(left=0.02, right=0.98, bottom=0.05, top=0.9)\n",
    "colors = ['#4EACC5', '#FF9C34', '#4E9A06']\n",
    "\n",
    "# We want to have the same colors for the same cluster from the\n",
    "# MiniBatchKMeans and the KMeans algorithm. Let's pair the cluster centers per\n",
    "# closest one.\n",
    "stepSize = math.floor(X.shape[0]/1000) # plot 10000 points only\n",
    "Xplot = X[0:-1:stepSize, :]\n",
    "k_means_cluster_centers = np.sort(k_means.cluster_centers_, axis=0)\n",
    "mbk_means_cluster_centers = np.sort(mbk.cluster_centers_, axis=0)\n",
    "k_means_labels = pairwise_distances_argmin(Xplot, k_means_cluster_centers)\n",
    "mbk_means_labels = pairwise_distances_argmin(Xplot, mbk_means_cluster_centers)\n",
    "order = pairwise_distances_argmin(k_means_cluster_centers,\n",
    "                                  mbk_means_cluster_centers)\n",
    "\n",
    "# KMeans\n",
    "ax = fig.add_subplot(1, 3, 1)\n",
    "for k, col in zip(range(n_clusters), colors):\n",
    "    my_members = k_means_labels == k\n",
    "    cluster_center = k_means_cluster_centers[k]\n",
    "    ax.plot(Xplot[my_members, 0], Xplot[my_members, 1], 'w',\n",
    "            markerfacecolor=col, marker='.')\n",
    "    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "            markeredgecolor='k', markersize=6)\n",
    "ax.set_title('KMeans')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "plt.text(-3.5, 1.8,  'train time: %.2fs\\ninertia: %f' % (\n",
    "    t_batch, k_means.inertia_))\n",
    "\n",
    "# MiniBatchKMeans\n",
    "ax = fig.add_subplot(1, 3, 2)\n",
    "for k, col in zip(range(n_clusters), colors):\n",
    "    my_members = mbk_means_labels == order[k]\n",
    "    cluster_center = mbk_means_cluster_centers[order[k]]\n",
    "    ax.plot(Xplot[my_members, 0], Xplot[my_members, 1], 'w',\n",
    "            markerfacecolor=col, marker='.')\n",
    "    ax.plot(cluster_center[0], cluster_center[1], 'o', markerfacecolor=col,\n",
    "            markeredgecolor='k', markersize=6)\n",
    "ax.set_title('MiniBatchKMeans')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "plt.text(-3.5, 1.8, 'train time: %.2fs\\ninertia: %f' %\n",
    "         (t_mini_batch, mbk.inertia_))\n",
    "\n",
    "# Initialise the different array to all False\n",
    "different = (mbk_means_labels == 4)\n",
    "ax = fig.add_subplot(1, 3, 3)\n",
    "\n",
    "for k in range(n_clusters):\n",
    "    different += ((k_means_labels == k) != (mbk_means_labels == order[k]))\n",
    "\n",
    "identic = np.logical_not(different)\n",
    "ax.plot(Xplot[identic, 0], Xplot[identic, 1], 'w',\n",
    "        markerfacecolor='#bbbbbb', marker='.')\n",
    "ax.plot(Xplot[different, 0], Xplot[different, 1], 'w',\n",
    "        markerfacecolor='m', marker='.')\n",
    "ax.set_title('Difference')\n",
    "ax.set_xticks(())\n",
    "ax.set_yticks(())\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
