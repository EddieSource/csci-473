{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "84f30d75",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Lab 5: SVM recap, Decision Trees and Random Forests\n",
    "### Date: March 9th, 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e9c25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import metrics, model_selection, tree\n",
    "import matplotlib.pyplot as plt\n",
    "BLUE, ORANGE, GRAY = '#57B5E8', '#E69E00', '#646369'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867aa947",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    N : the number of data points\n",
    "\n",
    "Output:\n",
    "    X, y : the features and targets of shapes (N,2) and (N, )\n",
    "\"\"\"\n",
    "def sample_bimodal_data(N):\n",
    "    \n",
    "    # The two modes and covariances.\n",
    "    mu1 = np.asarray([1, 0])\n",
    "    mu2 = np.asarray([-1, 0])\n",
    "    \n",
    "    cov1 = 2 * np.identity(2)\n",
    "    cov2 = 2 * np.identity(2)\n",
    "    \n",
    "    N1 = N//2   # Number of points in first class.\n",
    "    N2 = N - N1 # Number of points in second class.\n",
    "    \n",
    "    # Sample the random points.\n",
    "    X1 = np.random.multivariate_normal(mu1, cov1, N1)\n",
    "    X2 = np.random.multivariate_normal(mu2, cov2, N2)\n",
    "    Y1 = np.zeros(N1)\n",
    "    Y2 = np.ones(N2)\n",
    "    \n",
    "    # Combine the data.\n",
    "    X = np.vstack((X1, X2))\n",
    "    Y = np.concatenate((Y1, Y2), axis = None)\n",
    "    \n",
    "    return X,Y\n",
    "\n",
    "# Plot the sample data.\n",
    "N = 100\n",
    "X,Y = sample_bimodal_data(N)\n",
    "\n",
    "plt.figure(1)\n",
    "plt.scatter(X[:N//2, 0], X[:N//2, 1], label = 'Class 0')\n",
    "plt.scatter(X[N - N//2:, 0], X[N - N//2:, 1], label = 'Class 1')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Sample Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0cf29a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First get the data and split it into training and testing.\n",
    "\n",
    "N = 100\n",
    "X, Y = sample_bimodal_data(N)\n",
    "\n",
    "# Use a 70/30 split\n",
    "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(X, Y, test_size = 0.30, random_state = 981)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc901849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the SVM model to use with a slack variable\n",
    "svm = LinearSVC(C = 1e-10, dual = False)\n",
    "svm.fit(X_train, Y_train)\n",
    "svmpred = svm.predict(X_val)\n",
    "acc = metrics.accuracy_score(Y_val, svmpred)\n",
    "print('SVM accuracy = {:0.1f}%'.format(100*acc))\n",
    "\n",
    "plt.figure(2)\n",
    "I = svmpred == 0\n",
    "plt.scatter(X_val[I, 0], X_val[I, 1], label = 'Predicted class 0')\n",
    "I = svmpred == 1\n",
    "plt.scatter(X_val[I, 0], X_val[I, 1], label = 'Predicted class 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da269e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Input:\n",
    "    N : the number of data points\n",
    "\n",
    "Output:\n",
    "    X, y : the features and targets of shapes (N,2) and (N, )\n",
    "\"\"\"\n",
    "def gen_data1(N):\n",
    "    N1 = N//2\n",
    "    N2 = N - N1\n",
    "    t = np.linspace(0, 2*np.pi, N1)\n",
    "    \n",
    "    X1 = np.zeros((N1, 2))\n",
    "    X1[:,0] = 4*np.cos(t) + 0.1*np.random.randn(N1)\n",
    "    X1[:,1] = 4*np.sin(t) + 0.1*np.random.randn(N1)\n",
    "    y1 = np.zeros(N1)\n",
    "    \n",
    "    X2 = np.random.randn(2*N2)\n",
    "    X2 = X2.reshape((N2, 2))\n",
    "    y2 = np.ones(N2)\n",
    "\n",
    "    # Combine the data.\n",
    "    X = np.vstack((X1, X2))\n",
    "    y = np.concatenate((y1, y2), axis = None) # axis = None means that arrays flattened before use\n",
    "    \n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8b0137",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot the data.\n",
    "N = 1000\n",
    "X, Y = gen_data1(N)\n",
    "\n",
    "plt.figure(3)\n",
    "plt.scatter(X[:N//2, 0], X[:N//2, 1], label = 'Class 0')\n",
    "plt.scatter(X[N - N//2:, 0], X[N - N//2:, 1], label = 'Class 1')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$x_1$')\n",
    "plt.ylabel(r'$x_2$')\n",
    "plt.title('Sample Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ecb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the different SVM models to use\n",
    "# Use a 70/30 split\n",
    "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(X, Y, test_size = 0.3, random_state = 981)\n",
    "svm = LinearSVC(C = 1e+10, dual = False)\n",
    "svm.fit(X_train, Y_train)\n",
    "svmpred = svm.predict(X_val)\n",
    "acc = metrics.accuracy_score(Y_val, svmpred)\n",
    "print('SVM accuracy = {:0.1f}%'.format(100*acc))\n",
    "\n",
    "plt.figure(2)\n",
    "I = svmpred == 0\n",
    "plt.scatter(X_val[I, 0], X_val[I, 1], label = 'predicted class 0')\n",
    "I = svmpred == 1\n",
    "plt.scatter(X_val[I, 0], X_val[I, 1], label = 'prediced class 1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e943d6",
   "metadata": {},
   "source": [
    "Here the data is not linearly separable although we can very clearly see some separation. If we transform the data by only looking at the radius, then we would be able to linearly separate the data. We will visit this in the next lecture when we talk about kernel SVM's which are much more flexible models and can handle a wider array of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7e306d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cart2pol(x, y):\n",
    "    rho = np.sqrt(x**2 + y**2)\n",
    "    phi = np.arctan2(y, x)\n",
    "    return (rho, phi)\n",
    "\n",
    "pX = np.vstack(cart2pol(X[:, 0], X[:, 1])).T\n",
    "print(pX.shape)\n",
    "plt.figure(4)\n",
    "plt.scatter(pX[:N//2, 0], pX[:N//2, 1], label = 'Class 0')\n",
    "plt.scatter(pX[N - N//2:, 0], pX[N - N//2:, 1], label = 'Class 1')\n",
    "plt.legend()\n",
    "plt.xlabel(r'$p_1$ (radius)')\n",
    "plt.ylabel(r'$p_2$ (angle)')\n",
    "plt.title('Sample Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "691b8e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the different SVM models to use\n",
    "# Use a 70/30 split\n",
    "X_train, X_val, Y_train, Y_val = model_selection.train_test_split(pX, Y, test_size = 0.3, random_state = 981)\n",
    "svm = LinearSVC(C = 1e+10, dual = False)\n",
    "svm.fit(X_train, Y_train)\n",
    "svmpred = svm.predict(X_val)\n",
    "acc = metrics.accuracy_score(Y_val, svmpred)\n",
    "print('SVM accuracy = {:0.1f}%'.format(100*acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a8a76d",
   "metadata": {},
   "source": [
    "## Moving on to decision trees.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec244cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets first create a basic decision tree for a very basic dataset!\n",
    "\n",
    "#Our training data X is composed of two features, and our predictions are binary (0 or 1)\n",
    "X = np.array([[0, 0], [1, 1], [2,2], [2,3], [0,2], [1,2]])\n",
    "Y = np.array([0, 1, 0, 1, 0, 0 ])\n",
    "\n",
    "def plot_data(X, Y):\n",
    "    fig, ax = plt.subplots(figsize=(1, 6), dpi=200)\n",
    "    ax.set_aspect(1.3)\n",
    "    ax.scatter(X[:, 0], X[:, 1], s=18, facecolors='none',\n",
    "               edgecolors=np.array([BLUE, ORANGE])[Y])\n",
    "    ax.tick_params(\n",
    "        bottom=True, left=True, labelleft=False, labelbottom=False)\n",
    "    ax.text(0, 3.2, 'Training Data', color=GRAY, fontsize=9)\n",
    "    ax.set_xlabel(r'$X_0$')\n",
    "    ax.set_ylabel(r'$X_1$')\n",
    "    return fig, ax\n",
    "\n",
    "_, _ = plot_data(X, Y)\n",
    "\n",
    "# Default splitting is done using Gini impurity, we will use entropy for our exercise.\n",
    "clf = tree.DecisionTreeClassifier(criterion=\"entropy\") \n",
    "clf.fit(X,Y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d70fa5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_X = 3 + 2.5 * np.random.randn(50, 2)\n",
    "clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191da7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also see the probability with which the classifier assigns each class for every sample\n",
    "clf.predict_proba(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5810ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The decision tree seems to be pretty confident on all of its predictions. Any ideas how we can confuse it?\n",
    "# HINT : Here probabilty can be thought of as the number of samples in the leaf node with the same class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2301525",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was just a programmatic dataset, lets try the decision tree on a more meaningful one\n",
    "from sklearn.datasets import load_digits\n",
    "import graphviz\n",
    "\n",
    "# We use the digits dataset provided by sklearn, this is similar to MNIST but much coarser (8x8) and \n",
    "# thus a lot lighter than MNIST (28x28)\n",
    "dataset = load_digits()\n",
    "X, Y = dataset.data, dataset.target\n",
    "idxs = np.arange(0, len(X))\n",
    "np.random.shuffle(idxs)\n",
    "train_idxs,test_idxs = idxs[:1500], idxs[1500:]\n",
    "train_X, train_Y = X[train_idxs], Y[train_idxs]\n",
    "test_X, test_Y = X[test_idxs], Y[test_idxs]\n",
    "plt.gray()\n",
    "plt.matshow(dataset.images[100])\n",
    "print(dataset.target[100])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ea7304",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = tree.DecisionTreeClassifier(criterion='entropy')\n",
    "clf = clf.fit(train_X, train_Y)\n",
    "dot_data = tree.export_graphviz(clf, out_file='graph.dot', \n",
    "                      feature_names=dataset.feature_names,  \n",
    "                      class_names=str(dataset.target_names),  \n",
    "                      filled=True, rounded=True,  \n",
    "                      special_characters=True) \n",
    "graph = graphviz.Source(dot_data)\n",
    "!dot -Tpng graph.dot -o graph.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f52b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict using the fitted decision tree\n",
    "preds = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8b8286",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(preds == test_Y)/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27233c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This was your vanilla Decision trees, now lets look at Bagging.\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "# Remember, Bagging is just using an ensemble of decision trees to add more variance to your model.\n",
    "# So we simply wrap our original DecisionTreeClassifier with a BaggingClassifier module.\n",
    "clf = BaggingClassifier(base_estimator=tree.DecisionTreeClassifier(criterion='gini'),\n",
    "                       n_estimators=100, max_samples=1.0, max_features=0.5,bootstrap=True)\n",
    "clf = clf.fit(train_X, train_Y)\n",
    "# What would setting n_estimators as 1 mean?\n",
    "# Changing the value for n_estimators changes our accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295bcd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491d39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(preds == test_Y)/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ad791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forests\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Now we try RandomForests on the same data. Again, RandomForests is just a method to ensemble your base models\n",
    "# and will be used in the same way bagging was.\n",
    "clf = RandomForestClassifier(n_estimators=100, max_samples=1.0, max_features=0.5,bootstrap=True, criterion='gini')\n",
    "clf.fit(train_X, train_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550a607d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = clf.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db28b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(preds == test_Y)/len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1aad7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we implement gradient boosting, in particular the Adaboost algorithm.\n",
    "# Remember, gradient boosting algorithms involve iteratively improving the decision trees\n",
    "# and hence involve a learning rate similar to logistic regressions.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_gaussian_quantiles\n",
    "\n",
    "# Construct dataset\n",
    "X1, y1 = make_gaussian_quantiles(\n",
    "    cov=2.0, n_samples=200, n_features=2, n_classes=2, random_state=1\n",
    ")\n",
    "X2, y2 = make_gaussian_quantiles(\n",
    "    mean=(3, 3), cov=1.5, n_samples=300, n_features=2, n_classes=2, random_state=1\n",
    ")\n",
    "X = np.concatenate((X1, X2))\n",
    "y = np.concatenate((y1, -y2 + 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8f709a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training points\n",
    "plot_colors = \"br\"\n",
    "plot_step = 0.02\n",
    "class_names = \"AB\"\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(\n",
    "        X[idx, 0],\n",
    "        X[idx, 1],\n",
    "        c=c,\n",
    "        cmap=plt.cm.Paired,\n",
    "        s=20,\n",
    "        edgecolor=\"k\",\n",
    "        label=\"Class %s\" % n,\n",
    "    )\n",
    "    plt.xlabel(r'$X_0$')\n",
    "    plt.ylabel(r'$X_1$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22eb205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and fit an AdaBoosted decision tree\n",
    "bdt = AdaBoostClassifier(\n",
    "    tree.DecisionTreeClassifier(max_depth=1), algorithm=\"SAMME\", n_estimators=2000, learning_rate=1\n",
    ")\n",
    "bdt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acca1c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the decision boundaries\n",
    "plt.subplot(111)\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(\n",
    "    np.arange(x_min, x_max, plot_step), np.arange(y_min, y_max, plot_step)\n",
    ")\n",
    "\n",
    "Z = bdt.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "plt.axis(\"tight\")\n",
    "\n",
    "# Plot the training points\n",
    "for i, n, c in zip(range(2), class_names, plot_colors):\n",
    "    idx = np.where(y == i)\n",
    "    plt.scatter(\n",
    "        X[idx, 0],\n",
    "        X[idx, 1],\n",
    "        c=c,\n",
    "        cmap=plt.cm.Paired,\n",
    "        s=20,\n",
    "        edgecolor=\"k\",\n",
    "        label=\"Class %s\" % n,\n",
    "    )\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.title(\"Decision Boundary\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fee175",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
