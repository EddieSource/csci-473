{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9cdc35c",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Lab 3: Regularization & Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598a22e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's import some packages we'll need.\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "import pandas as pd\n",
    "import autograd.numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "matplotlib.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960f5e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data:\n",
    "sata = np.genfromtxt('extendedHousingSata.csv', delimiter=',')\n",
    "print(sata.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be77c26",
   "metadata": {},
   "source": [
    "## This new sata set has two additional features. We inspect the correlations between the features to see if we find anything unusual."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a31a7e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check correlations between different features of our data\n",
    "np.corrcoef(sata[:,2], sata[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64cfa91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we sample datapoints to see how variable our betas are!\n",
    "nSample = 200 # Sub-Sampling points\n",
    "nPop = len(sata)  # The full population\n",
    "nReps = 1000 # How many times are we doing this?\n",
    "# Declare a structure to store our betas\n",
    "betasOLS = np.empty([np.size(sata,axis=1),nReps])*np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e476ebbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ii in range(nReps): \n",
    "    # Sample nSample number of indexes from our data randomly\n",
    "    sampleIndices = np.random.randint(0,nPop-1,nSample) \n",
    "    trainSamples = sata[sampleIndices,:]\n",
    "    # House price, our \"label\" is the fourth feature\n",
    "    y = trainSamples[:,4]\n",
    "    # Use remaining features for training\n",
    "    x = np.delete(trainSamples, 4, 1)\n",
    "    # The data is varying in scale and variance, subtracting the mean and dividing by the standard deviation is \n",
    "    # a common technique use to \"normalize\" the data\n",
    "    x_norm = (x - np.mean(x, axis=0))/np.std(x, axis=0)\n",
    "    y_norm = (y - np.mean(y, axis=0))/np.std(y, axis=0)\n",
    "    # Run OLS and store betas:\n",
    "    ols = LinearRegression().fit(x_norm,y_norm)\n",
    "    betasOLS[:,ii] = np.concatenate((np.array([ols.intercept_]), ols.coef_),axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d858c65f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nBins = 31\n",
    "plt.xlabel(r'$\\beta_{OLS}$')\n",
    "plt.ylabel('Count')\n",
    "plt.title(r'Distribution of $\\beta_{OLS}$')\n",
    "plt.hist(betasOLS[5,:],nBins);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09b0cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets pick a value for lambda randomly\n",
    "lambd = 10\n",
    "betasRidge = np.empty([np.size(sata,axis=1),nReps])*np.NaN\n",
    "for ii in range(nReps):\n",
    "    # We repreat our training process, this time using the Ridge model instead of the vanilla LinearRegression\n",
    "    sampleIndices = np.random.randint(0,nPop-1,nSample) \n",
    "    trainSamples = sata[sampleIndices,:]\n",
    "    y = trainSamples[:,4]\n",
    "    x = np.delete(trainSamples, 4, 1)\n",
    "    x_norm = (x - np.mean(x, axis=0))/np.std(x, axis=0)\n",
    "    y_norm = (y - np.mean(y, axis=0))/np.std(y, axis=0)\n",
    "    # Run Ridge and store betas:\n",
    "    ridge = Ridge(alpha=lambd).fit(x_norm,y_norm)\n",
    "    betasRidge[:,ii] = np.concatenate((np.array([ridge.intercept_]), ridge.coef_),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88391f06",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nBins = 31\n",
    "plt.xlabel(r'$\\beta_{Ridge}$')\n",
    "plt.ylabel('Count')\n",
    "plt.title(r'Distribution of $\\beta_{Ridge}$')\n",
    "plt.hist(betasRidge[5,:],nBins);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b90567",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 10\n",
    "betasOLS = np.empty([np.size(sata,axis=1),nReps])*np.NaN\n",
    "betasRidge = np.empty([np.size(sata,axis=1),nReps])*np.NaN\n",
    "for ii in range(nReps):\n",
    "    # We repreat our training process, this time using the Ridge model instead of the vanilla LinearRegression\n",
    "    sampleIndices = np.random.randint(0,nPop-1,nSample) \n",
    "    trainSamples = sata[sampleIndices,:]\n",
    "    y = trainSamples[:,4]\n",
    "    x = np.delete(trainSamples, 4, 1)\n",
    "    x_norm = (x - np.mean(x, axis=0))/np.std(x, axis=0)\n",
    "    y_norm = (y - np.mean(y, axis=0))/np.std(y, axis=0)\n",
    "    # Run OLS and store betas:\n",
    "    ols = LinearRegression().fit(x_norm,y_norm)\n",
    "    betasOLS[:,ii] = np.concatenate((np.array([ols.intercept_]), ols.coef_),axis=0)\n",
    "    # Run Ridge and store betas:\n",
    "    ridge = Ridge(alpha=lambd).fit(x_norm,y_norm)\n",
    "    betasRidge[:,ii] = np.concatenate((np.array([ridge.intercept_]), ridge.coef_),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df1e621e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Add a scatter plot for values of betas before and after regularization \n",
    "plt.scatter(betasOLS, betasRidge)\n",
    "plt.xlabel(r'$\\beta_{OLS}$')\n",
    "plt.ylabel(r'$\\beta_{Ridge}$')\n",
    "plt.title(r'Spread of $\\beta_{OLS}$ and $\\beta_{Ridge}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba8e40e",
   "metadata": {},
   "source": [
    "## But how to we choose a value for $\\lambda$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f688ac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add hyperparameter tuning to the above example\n",
    "lambdas = np.linspace(0,30,1001)\n",
    "cont = np.empty([len(lambdas),2])*np.NaN # [lambda error]\n",
    "y = sata[:,4]\n",
    "x = np.delete(sata, 4, 1)\n",
    "# Again we normalize our data\n",
    "x_norm = (x - np.mean(x, axis=0))/np.std(x, axis=0)\n",
    "y_norm = (y - np.mean(y, axis=0))/np.std(y, axis=0)\n",
    "xTrain, xTest, yTrain, yTest = model_selection.train_test_split(x_norm, y_norm.reshape(-1,1), test_size=0.2, random_state=0)\n",
    "for ii in range(len(lambdas)):\n",
    "    ridgeModel = Ridge(alpha=lambdas[ii]).fit(xTrain, yTrain)\n",
    "    cont[ii,0] = lambdas[ii]\n",
    "    error = metrics.mean_squared_error(yTest,ridgeModel.predict(xTest))\n",
    "    cont[ii,1] = error\n",
    "\n",
    "plt.plot(cont[:,0],cont[:,1])\n",
    "plt.xlabel('Lambda')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Ridge regression')\n",
    "plt.show()\n",
    "print('Optimal lambda:',lambdas[np.argmax(cont[:,1]==np.min(cont[:,1]))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f706fb1",
   "metadata": {},
   "source": [
    "## That was Ridge regression, but we can use Lasso as well!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d79c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We repeat our initial experiment again\n",
    "nSample = 200 # Sub-Sampling points\n",
    "nPop = len(sata)  # The full population\n",
    "nReps = 1000 # How many times are we doing this?\n",
    "# Declare a structure to store our betas\n",
    "betasOLS = np.empty([np.size(sata,axis=1),nReps])*np.NaN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96cba3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is OLS repeated, to compare\n",
    "for ii in range(nReps): \n",
    "    # Sample nSample number of indexes from our data randomly\n",
    "    sampleIndices = np.random.randint(0,nPop-1,nSample) \n",
    "    trainSamples = sata[sampleIndices,:]\n",
    "    # House price, our \"label\" is the fourth feature\n",
    "    y = trainSamples[:,4]\n",
    "    # Use remaining features for training\n",
    "    x = np.delete(trainSamples, 4, 1)\n",
    "    # The data is varying in scale and variance, subtracting the mean and dividing by the standard deviation is \n",
    "    # a common technique use to \"normalize\" the data\n",
    "    x_norm = (x - np.mean(x, axis=0))/np.std(x, axis=0)\n",
    "    y_norm = (y - np.mean(y, axis=0))/np.std(y, axis=0)\n",
    "    # Run OLS and store betas:\n",
    "    ols = LinearRegression().fit(x_norm,y_norm)\n",
    "    betasOLS[:,ii] = np.concatenate((np.array([ols.intercept_]), ols.coef_),axis=0)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793341f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nBins = 31\n",
    "plt.xlabel(r'$\\beta_{Lasso}$')\n",
    "plt.ylabel('Count')\n",
    "plt.title(r'Distribution of $\\beta_{Lasso}$')\n",
    "plt.hist(betasOLS[6,:],nBins);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cc52b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets pick a value for lambda randomly\n",
    "lambd = 1e-2\n",
    "betasLasso = np.empty([np.size(sata,axis=1),nReps])*np.NaN\n",
    "for ii in range(nReps):\n",
    "    # We repreat our training process, this time using the Ridge model instead of the vanilla LinearRegression\n",
    "    sampleIndices = np.random.randint(0,nPop-1,nSample) \n",
    "    trainSamples = sata[sampleIndices,:]\n",
    "    y = trainSamples[:,4]\n",
    "    x = np.delete(trainSamples, 4, 1)\n",
    "    x_norm = (x - np.mean(x, axis=0))/np.std(x, axis=0)\n",
    "    y_norm = (y - np.mean(y, axis=0))/np.std(y, axis=0)\n",
    "    # Run Ridge and store betas:\n",
    "    lasso = Lasso(alpha=lambd).fit(x_norm,y_norm)\n",
    "    betasLasso[:,ii] = np.concatenate((np.array([lasso.intercept_]), lasso.coef_),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c425f0f5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nBins = 31\n",
    "plt.hist(betasLasso[6,:],nBins);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b2c68cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambd = 1e-2\n",
    "betasOLS = np.empty([np.size(sata,axis=1),nReps])*np.NaN\n",
    "betasLasso = np.empty([np.size(sata,axis=1),nReps])*np.NaN\n",
    "for ii in range(nReps):\n",
    "    # We repreat our training process, this time using the Ridge model instead of the vanilla LinearRegression\n",
    "    sampleIndices = np.random.randint(0,nPop-1,nSample) \n",
    "    trainSamples = sata[sampleIndices,:]\n",
    "    y = trainSamples[:,4]\n",
    "    x = np.delete(trainSamples, 4, 1)\n",
    "    x_norm = (x - np.mean(x, axis=0))/np.std(x, axis=0)\n",
    "    y_norm = (y - np.mean(y, axis=0))/np.std(y, axis=0)\n",
    "    # Run OLS and store betas:\n",
    "    ols = LinearRegression().fit(x_norm,y_norm)\n",
    "    betasOLS[:,ii] = np.concatenate((np.array([ols.intercept_]), ols.coef_),axis=0)\n",
    "    # Run Ridge and store betas:\n",
    "    lasso = Lasso(alpha=lambd).fit(x_norm,y_norm)\n",
    "    betasLasso[:,ii] = np.concatenate((np.array([lasso.intercept_]), lasso.coef_),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcbf667",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Add a scatter plot for values of betas before and after regularization\n",
    "plt.scatter(betasOLS, betasLasso)\n",
    "plt.xlabel(r'$\\beta_{OLS}$')\n",
    "plt.ylabel(r'$\\beta_{Lasso}$');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247baff",
   "metadata": {},
   "source": [
    "# Using Autograd\n",
    "\n",
    "Autograd is a package for automatic differentiation.  ``autograd.numpy`` is a wrapper for Numpy which contains the same basic operations.  In most machine learning algorithms we need the gradient of a certain loss function to fit our model to the given data.  Autograd computes this gradient automatically for us so that we may use methods such as gradient descent.  A popular function used in logistic regression as well as neural nets is the sigmoid function\n",
    "\n",
    "$$\n",
    "\\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$\n",
    "\n",
    "The derivative is given by\n",
    "\n",
    "$$\n",
    "\\sigma'(x) = \\frac{e^{-x}}{(1 + e^{-x})^2} = \\sigma(x) (1 - \\sigma(x))\n",
    "$$\n",
    "\n",
    "We could either compute this derivative by hand and then hard code it explicitly, or we could just call autograd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3da6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sigmoid function.\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# Hard code the gradient of the sigmoid function.\n",
    "def grad_sigmoid(x):\n",
    "    y = sigmoid(x)\n",
    "    return y * (1 - y)\n",
    "\n",
    "# Evaluate the gradient using autograd.\n",
    "grad_sig = grad(sigmoid)\n",
    "\n",
    "# Plot the two gradients side by side.\n",
    "x = np.linspace(-4, 4, 100)\n",
    "\n",
    "\n",
    "# We'll have 2 plots side by side.\n",
    "fig = plt.figure(3, figsize = (10, 5))\n",
    "\n",
    "# First plot the autograd derivative.\n",
    "ax1 = plt.subplot(121) # create a figure with 1 row and 2 columns and get the axis \n",
    "y1 = np.asarray([grad_sig(x[i]) for i in range(len(x))])\n",
    "ax1.plot(x, y1, 'b')\n",
    "ax1.set_xlabel(r'$x$')\n",
    "ax1.set_ylabel(r'$\\sigma\\'(x)$')\n",
    "ax1.set_title(r'Autograd $\\sigma\\'(x)$')\n",
    "\n",
    "# Second plot the hard-coded derivative.\n",
    "ax2 = plt.subplot(122)\n",
    "y2 = grad_sigmoid(x)\n",
    "ax2.plot(x, y2, 'r')\n",
    "ax2.set_xlabel(r'$x$')\n",
    "ax2.set_ylabel(r'$\\sigma\\'(x)$')\n",
    "ax2.set_title(r'Hard coded $\\sigma\\'(x)$')\n",
    "\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51e1b17",
   "metadata": {},
   "source": [
    "## Part 1: Logistic Regression using Synthetic Data and Autograd\n",
    "\n",
    "We will play around with the logistic regression model implemented from scratch and trained using autograd on synthetic data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b791853a",
   "metadata": {},
   "source": [
    "### Generate the data and plot it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c220e40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the plots in the remainder of this notebook we will use a widget to get interactive and better looking plots! \n",
    "# follow these instructions to set up matplotlib widget for Jupyter Lab\n",
    "# https://github.com/matplotlib/jupyter-matplotlib#installation\n",
    "# Specifically you will need to install the widget using : conda install -c conda-forge ipympl\n",
    "# To enable the widget run the following two commands :\n",
    "# jupyter nbextension install --py --symlink --sys-prefix --overwrite ipympl\n",
    "#jupyter nbextension enable --py --sys-prefix ipympl\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "import numpy\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# random_seed = numpy.random.randint(0,100)\n",
    "random_seed = 65\n",
    "\n",
    "x, y = make_blobs(n_samples=2000, centers=2, n_features=2, random_state=random_seed)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.scatter(x[:,0], x[:,1], c=y)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0959aba",
   "metadata": {},
   "source": [
    "Now we have a set of data points $\\{(x_{1}, y_{1}), (x_{2}, y_{2}), ... , (x_{n}, y_{n})\\}$, where $x_{i} \\in R^{d}$ are the feature vectors and $y_{i} \\in \\{0, 1\\}$ are the class labels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ffc31",
   "metadata": {},
   "source": [
    "### Split the data into training set and validation set\n",
    "We consider the first 1500 points as our training set and the remaining 500 as our validation set. \n",
    "<b>Is this correct?</b> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e320534",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = x[:1500], y[:1500]\n",
    "x_val, y_val = x[1500:], y[1500:]\n",
    "\n",
    "# sanity check\n",
    "assert len(x_train) == len(y_train) == 1500\n",
    "assert len(x_val) == len(y_val) == 500\n",
    "\n",
    "# should we check anything else?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7861b",
   "metadata": {},
   "source": [
    "### Build the model\n",
    "Logistic regression model outputs the posterior probability of the class label to be equal to 1. \n",
    "$$p_{+} = p(y = 1|x) = \\frac{1}{1 + e^{-w \\cdot x + b}},$$ \n",
    "where $w \\in R^{d}$ and $b \\in R$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a0b9796",
   "metadata": {},
   "source": [
    "Sigmoid function is used to convert the output of the linear operation into probabilities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a7ffcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_function(f):\n",
    "    x = numpy.linspace(-10,10,100)\n",
    "    y = f(x)\n",
    "    \n",
    "    return plt.plot(x,y)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plot_function(sigmoid)\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b27642",
   "metadata": {},
   "source": [
    "Now lets build the logistic regression model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891472bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_unit(x, w, b):\n",
    "    \"\"\"This function computes logistic unit as defined $p_{+}$ above\n",
    "    :param x: input x with n_dim features\n",
    "    :param w: weght vector\n",
    "    :param b: bias vector\n",
    "    \"\"\"\n",
    "    \n",
    "    # operator @ means matrix multiplication in python <3.5\n",
    "    # https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.matmul.html#numpy.matmul\n",
    "    \n",
    "    pre_sigmoid = x @ w + b\n",
    "    logit = sigmoid(pre_sigmoid)\n",
    "    \n",
    "    return logit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1192f92",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "The loss function of the logistic regression is given by: \n",
    "\n",
    "$$\\mathcal{L}(x,y) = - ( y \\cdot \\log(p_{+}) + (1 - y) \\cdot \\log(1 - p_{+}) )$$\n",
    "\n",
    "Note that $p_{+}$ depends on $x$ and $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c00e99c",
   "metadata": {},
   "outputs": [],
   "source": [
    "NEAR_ZERO = 1e-12\n",
    "\n",
    "def loss_function(x, y, w, b):\n",
    "    \"\"\"This function computes the loss (distance)\n",
    "    :param logits: output from logistic unit\n",
    "    :param labels: target label\n",
    "    \"\"\"\n",
    "    logits = logistic_unit(x,w,b)\n",
    "    labels = y\n",
    "    \n",
    "    loss = -(labels * numpy.log(logits + NEAR_ZERO) + (1 - labels) * numpy.log(1 - logits + NEAR_ZERO))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3fb617",
   "metadata": {},
   "source": [
    "Now lets do a sanity check and compute the loss between the target and prediction given a randomly initialized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd35da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "n_dim=2\n",
    "w0 = 0.01 * numpy.random.randn(n_dim)\n",
    "b0 = 0.0\n",
    "\n",
    "inp = x_train[0]\n",
    "label = y_train[0]\n",
    "\n",
    "out = logistic_unit(inp, w0, b0)\n",
    "assert (out < 1) and (out > 0)  # why?\n",
    "\n",
    "loss = loss_function(inp, label, w0, b0)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c812a52d",
   "metadata": {},
   "source": [
    "### Use Autograd for computing the derivatives and train the model\n",
    "As discussed in the previous lab, with Autograd we do not need to compute the gradients by hand and code it. This is very handy when we have huge DAG (Directed Acyclic Graph) computations.\n",
    "There is an active line of research in automatic differentiation, curious students are adviced to read this:\n",
    "http://jmlr.org/papers/volume18/17-468/17-468.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8a841e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install autograd:\n",
    "#!conda install -c conda-forge autograd\n",
    "\n",
    "import autograd.numpy as numpy\n",
    "import autograd.numpy.random as npr\n",
    "\n",
    "from autograd import elementwise_grad\n",
    "\n",
    "import scipy.optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d85d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_grad_w = elementwise_grad(loss_function, 2) # partial derivative wrt the weights w (3rd input)\n",
    "loss_grad_b = elementwise_grad(loss_function, 3) # partial derivative wrt the bias b (4th input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61077bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 10000\n",
    "eta = .0001   # Learning rate\n",
    "w = numpy.copy(w0)\n",
    "b = numpy.copy(b0)\n",
    "\n",
    "loss_log = []\n",
    "\n",
    "for ni in tqdm(range(n_iter)):\n",
    "    dw = loss_grad_w(x_train, y_train, w, b)\n",
    "    db = loss_grad_b(x_train, y_train, w, b)\n",
    "    w -= eta * dw\n",
    "    b -= eta * db\n",
    "    \n",
    "    loss = numpy.mean(loss_function(x_train, y_train, w, b))\n",
    "    loss_log.append(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c493b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets plot the running element-wise loss value\n",
    "plt.figure()\n",
    "plt.plot(loss_log)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Log Loss')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bc87bc7",
   "metadata": {},
   "source": [
    "### Plot the data: the actual data and the model (hyperplane)\n",
    "We start with first defining some visualization routines and then we do the actual plotting. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92e72f1f",
   "metadata": {},
   "source": [
    "Some visualization routines: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02731a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize data \n",
    "def vis_data(x, y = None, c='r'):\n",
    "    if y is None: \n",
    "        y = [None] * len(x)\n",
    "    for x_, y_ in zip(x, y):\n",
    "        if y_ is None:\n",
    "            plt.plot(x_[0], x_[1], 'o', markerfacecolor='none', markeredgecolor=c)\n",
    "        else:\n",
    "            plt.plot(x_[0], x_[1], c+'o' if y_ == 0 else c+'+')\n",
    "    plt.grid('on')\n",
    "    \n",
    "def vis_hyperplane(w, b, typ='k--'):\n",
    "\n",
    "    lim0 = plt.gca().get_xlim()\n",
    "    lim1 = plt.gca().get_ylim()\n",
    "    m0, m1 = lim0[0], lim0[1]\n",
    "\n",
    "    intercept0 = -(w[0] * m0 + b)/w[1]\n",
    "    intercept1 = -(w[0] * m1 + b)/w[1]\n",
    "    \n",
    "    plt1, = plt.plot([m0, m1], [intercept0, intercept1], typ)\n",
    "\n",
    "    plt.gca().set_xlim(lim0)\n",
    "    plt.gca().set_ylim(lim1)\n",
    "        \n",
    "    return plt1\n",
    "\n",
    "def vis_decision_boundary_contour(w, b, typ='k--'):\n",
    "    \n",
    "    lim0 = plt.gca().get_xlim()\n",
    "    lim1 = plt.gca().get_ylim()\n",
    "    \n",
    "    x_ = numpy.linspace(lim0[0], lim0[1], 100)\n",
    "    y_ = numpy.linspace(lim1[0], lim1[1], 100)\n",
    "    xx, yy = numpy.meshgrid(x_, y_)\n",
    "    \n",
    "    x_tra_ = numpy.concatenate([xx.ravel()[:,None], yy.ravel()[:,None]], axis=1)\n",
    "    \n",
    "    pred = logistic_unit(x_tra_, w, b)\n",
    "    plt1 = plt.contourf(xx, yy, pred.reshape(xx.shape), cmap=plot.cm.coolwarm, alpha=0.4)\n",
    "    \n",
    "    plt.colorbar(plt1)\n",
    "    \n",
    "    plt.gca().set_xlim(lim0)\n",
    "    plt.gca().set_ylim(lim1)\n",
    "        \n",
    "    return plt1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e59883",
   "metadata": {},
   "source": [
    "Now plot the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8bc524",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,8))\n",
    "\n",
    "vis_data(x, y, c='b')\n",
    "\n",
    "plt0 = vis_hyperplane(w0, b0, 'k-.')\n",
    "plt1 = vis_hyperplane(w, b, 'k--')\n",
    "plt.legend([plt0, plt1], [\n",
    "        'Initial: ${:.2} x_1 + {:.2} x_2 + {:.2} = 0$'.format(*list(w0)+[b0]),\n",
    "        'Final: ${:.2} x_1 + {:.2} x_2 + {:.2} = 0$'.format(*list(w)+[b])],\n",
    "           loc='best')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
