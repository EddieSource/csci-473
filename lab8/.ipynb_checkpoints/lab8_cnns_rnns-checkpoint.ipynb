{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Machine Learning (CSCI-UA.473)\n",
    "\n",
    "## Lab 8: Building Neural Networks using PyTorch - ConvNets, RNNs and LSTMs \n",
    "### Date: April 6th, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs)\n",
    "\n",
    "We will test the following assumptions pertaining to CNNs \n",
    "\n",
    "* Compositionality obtained using many layers\n",
    "* Locality + stationarity of images assumed by the convolutional layers\n",
    "* Invariance of object class to translations assumed by the pooling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from plot_lib import plot_data, plot_model, set_default\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "set_default()\n",
    "\n",
    "# function to count number of parameters\n",
    "def get_n_params(model):\n",
    "    np=0\n",
    "    for p in list(model.parameters()):\n",
    "        np += p.nelement()\n",
    "    return np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Dataset (MNIST)\n",
    "\n",
    "Load the MNIST handwritten digits dataset. We can use the PyTorch DataLoader utilities for this. This will download, shuffle, normalize data and arrange it in batches. Normalizing involves subtracting some coefficient (usually the mean) from each pixel values and dividing the resulting pixel values by another coefficient (usually the variance of the original pixel values). We also display some images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size  = 28*28   # images are 28x28 pixels\n",
    "output_size = 10      # there are 10 classes\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=64, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1600x600 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show some images\n",
    "plt.figure(figsize=(16, 6))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    plt.imshow(image.squeeze().numpy(), cmap='gray')\n",
    "    plt.axis('off');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the model classes\n",
    "For comparison purposes we will create two models classes: \n",
    "1. Multi-layer Perceptron\n",
    "2. Convolutional Neural Network\n",
    "\n",
    "Pay special attention to the order of the layer while creating CNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC2Layer(nn.Module):\n",
    "    def __init__(self, input_size, n_hidden, output_size):\n",
    "        super(FC2Layer, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(input_size, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, n_hidden), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(n_hidden, output_size), \n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.input_size)        \n",
    "        return self.network(x)\n",
    "    \n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size):\n",
    "        super(CNN, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        ## in_channels is image depth, out_channels is feature map; \n",
    "        ## image of RGB -> image of feature map\n",
    "        ## learning the value of filter in the convolution layer\n",
    "        self.conv1 = nn.Conv2d(in_channels=1, out_channels=n_feature, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(n_feature, n_feature, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(n_feature*4*4, 50)\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "        \n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x) # Will it make a difference if we apply the non-linearity after the pooling layer? no\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, kernel_size=2)\n",
    "        x = x.view(-1, self.n_feature*4*4) # this is where are flattening the 2D feature maps into a single 1D vector so as to be used by the subsequent fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_list = []\n",
    "def train(epoch, model, perm=torch.arange(0, 784).long()):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        # send data to device, where the \"device\" is either a GPU if it exists or a CPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        # permute pixels\n",
    "        data = data.view(-1, 28*28)\n",
    "        data = data[:, perm]\n",
    "        data = data.view(-1, 1, 28, 28)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        # forward pass through the model\n",
    "        output = model(data)\n",
    "        # forward pass through the cross-entropy loss function\n",
    "        loss = F.nll_loss(output, target)\n",
    "        # backward pass through the cross-entropy loss function and the model\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "            \n",
    "def test(model, perm=torch.arange(0, 784).long()):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    # don't want to calculate grad when testing\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            # send data to device, where the \"device\" is either a GPU if it exists or a CPU\n",
    "            data, target = data.to(device), target.to(device)\n",
    "\n",
    "            # permute pixels\n",
    "            data = data.view(-1, 28*28)\n",
    "            data = data[:, perm]\n",
    "            data = data.view(-1, 1, 28, 28)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss                                                               \n",
    "            pred = output.data.max(1, keepdim=True)[1] # get the index of the max log-probability                                                                 \n",
    "            correct += pred.eq(target.data.view_as(pred)).cpu().sum().item()\n",
    "\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        accuracy_list.append(accuracy)\n",
    "        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_loader.dataset),\n",
    "            accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a small MLP\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 6442\n",
      "Train Epoch: 0 [0/60000 (0%)]\tLoss: 2.340029\n",
      "Train Epoch: 0 [6400/60000 (11%)]\tLoss: 1.949853\n",
      "Train Epoch: 0 [12800/60000 (21%)]\tLoss: 1.229139\n",
      "Train Epoch: 0 [19200/60000 (32%)]\tLoss: 0.992325\n",
      "Train Epoch: 0 [25600/60000 (43%)]\tLoss: 0.660146\n",
      "Train Epoch: 0 [32000/60000 (53%)]\tLoss: 0.437907\n",
      "Train Epoch: 0 [38400/60000 (64%)]\tLoss: 0.757380\n",
      "Train Epoch: 0 [44800/60000 (75%)]\tLoss: 0.529483\n",
      "Train Epoch: 0 [51200/60000 (85%)]\tLoss: 0.525516\n",
      "Train Epoch: 0 [57600/60000 (96%)]\tLoss: 0.349456\n",
      "\n",
      "Test set: Average loss: 0.4263, Accuracy: 8743/10000 (87%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "n_hidden = 8 # number of hidden units\n",
    "\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_fnn)\n",
    "    test(model_fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a ConvNet with the same number of parameters\n",
    "Play around with the hyper-parameters to understand their relationship with model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings \n",
    "n_features = 6 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn)\n",
    "    test(model_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvNet performs better with the same number of parameters, thanks to its use of prior knowledge about images\n",
    "\n",
    "* Use of convolution: Locality and stationarity in images\n",
    "* Pooling: builds in some translation invariance\n",
    "\n",
    "### What happens if the assumptions are no longer true?\n",
    "Let us break the assumption of locality and permute the pixel within each image using an arbitrary permutation matrix. Also display the permuted images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm = torch.randperm(784)\n",
    "plt.figure(figsize=(16, 12))\n",
    "for i in range(10):\n",
    "    image, _ = train_loader.dataset.__getitem__(i)\n",
    "    # permute pixels\n",
    "    image_perm = image.view(-1, 28*28).clone()\n",
    "    image_perm = image_perm[:, perm]\n",
    "    image_perm = image_perm.view(-1, 1, 28, 28)\n",
    "    plt.subplot(4, 5, i + 1)\n",
    "    plt.imshow(image.squeeze().numpy())\n",
    "    plt.axis('off')\n",
    "    plt.subplot(4, 5, i + 11)\n",
    "    plt.imshow(image_perm.squeeze().numpy())\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNNs with permuted pixels\n",
    "What do you think will happen to CNNs when given permuted pixels as inputs? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings \n",
    "n_features = 6 # number of feature maps\n",
    "\n",
    "model_cnn = CNN(input_size, n_features, output_size)\n",
    "model_cnn.to(device)\n",
    "optimizer = optim.SGD(model_cnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_cnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_cnn, perm)\n",
    "    test(model_cnn, perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLPs with permuted pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 8    # number of hidden units\n",
    "\n",
    "model_fnn = FC2Layer(input_size, n_hidden, output_size)\n",
    "model_fnn.to(device)\n",
    "optimizer = optim.SGD(model_fnn.parameters(), lr=0.01, momentum=0.5)\n",
    "print('Number of parameters: {}'.format(get_n_params(model_fnn)))\n",
    "\n",
    "for epoch in range(0, 1):\n",
    "    train(epoch, model_fnn, perm)\n",
    "    test(model_fnn, perm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The ConvNet's performance drops when we permute the pixels, but the Fully-Connected Network's performance stays the same\n",
    "\n",
    "* ConvNet makes the assumption that pixels lie on a grid and are stationary/local\n",
    "* It loses performance when this assumption is wrong\n",
    "* The fully-connected network does not make this assumption\n",
    "* It does less well when it is true, since it doesn't take advantage of this prior knowledge\n",
    "* But it doesn't suffer when the assumption is wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(('NN image', 'CNN image',\n",
    "         'CNN scrambled', 'NN scrambled'),\n",
    "        accuracy_list, width=0.4)\n",
    "plt.ylim((min(accuracy_list)-5, 96))\n",
    "plt.ylabel('Accuracy [%]')\n",
    "for tick in plt.gca().xaxis.get_major_ticks():\n",
    "    tick.label.set_fontsize(12)\n",
    "plt.title('Performance comparison');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_cnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model_fnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Recurrent Neural Networks (RNNs)\n",
    "We now build a recurrent neural network and train it on a synthetic and simple sequence level task. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An example of many-to-one (sequence classification)\n",
    "\n",
    "This task is taken from Experiment 6(a) in the paper [Hochreiter & Schmidhuber (1997)](www.bioinf.jku.at/publications/older/2604.pdf).\n",
    "\n",
    "The goal is to classify sequences.\n",
    "Elements and targets are represented locally (input vectors with only one non-zero bit).\n",
    "The sequence starts with an `B`, ends with a `E` (the “trigger symbol”), and otherwise consists of randomly chosen symbols from the set `{a, b, c, d}` except for two elements at positions `t1` and `t2` that are either `X` or `Y`.\n",
    "For the `DifficultyLevel.HARD` case, the sequence length is randomly chosen between `100` and `110`, `t1` is randomly chosen between `10` and `20`, and `t2` is randomly chosen between `50` and `60`.\n",
    "There are `4` sequence classes `Q`, `R`, `S`, and `U`, which depend on the temporal order of `X` and `Y`.\n",
    "\n",
    "The rules are:\n",
    "\n",
    "```\n",
    "X, X -> Q,\n",
    "X, Y -> R,\n",
    "Y, X -> S,\n",
    "Y, Y -> U.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module that generates the sequence data according to the above described experiment\n",
    "from sequential_tasks import TemporalOrderExp6aSequence as QRSU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a data generator\n",
    "example_generator = QRSU.get_predefined_generator(\n",
    "    difficulty_level=QRSU.DifficultyLevel.EASY,\n",
    "    batch_size=16,\n",
    ")\n",
    "example_batch = example_generator[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the generated sequences and the labels in the current batch\n",
    "print(f'The return type is a {type(example_batch)} with length {len(example_batch)}.')\n",
    "print(f'The first item in the tuple is the batch of sequences with shape {example_batch[0].shape}.')\n",
    "print(f'The second item in the tuple is the corresponding batch of class labels with shape {example_batch[1].shape}.')\n",
    "print('The sequences generated and their corresponding labels in the batch are:')\n",
    "for i in range(example_batch[0].shape[0]):\n",
    "    input = example_batch[0][i]\n",
    "    output = example_batch[1][i]\n",
    "    sequence = example_generator.decode_x(input)\n",
    "    label = example_generator.decode_y(output)\n",
    "    print(sequence, ' ----- ', label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set the random seed for reproducible results\n",
    "torch.manual_seed(1)\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        # This just calls the base class constructor\n",
    "        super().__init__()\n",
    "        # Neural network layers assigned as attributes of a Module subclass\n",
    "        # have their parameters registered for training automatically.\n",
    "        self.rnn = torch.nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # The RNN also returns its hidden state but we don't use it.\n",
    "        # While the RNN can also take a hidden state as input, the RNN\n",
    "        # gets passed a hidden state initialized with zeros by default.\n",
    "        h = self.rnn(x)[0]\n",
    "        x = self.linear(h)\n",
    "        return x\n",
    "\n",
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = torch.nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.lstm(x)[0]\n",
    "        x = self.linear(h)\n",
    "        return x\n",
    "    \n",
    "    def get_states_across_time(self, x):\n",
    "        h_c = None\n",
    "        h_list, c_list = list(), list()\n",
    "        with torch.no_grad():\n",
    "            for t in range(x.size(1)):\n",
    "                h_c = self.lstm(x[:, [t], :], h_c)[1]\n",
    "                h_list.append(h_c[0])\n",
    "                c_list.append(h_c[1])\n",
    "            h = torch.cat(h_list)\n",
    "            c = torch.cat(c_list)\n",
    "        return h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data_gen, criterion, optimizer, device):\n",
    "    # Set the model to training mode. This will turn on layers that would\n",
    "    # otherwise behave differently during evaluation, such as dropout.\n",
    "    model.train()\n",
    "\n",
    "    # Store the number of sequences that were classified correctly\n",
    "    num_correct = 0\n",
    "\n",
    "    # Iterate over every batch of sequences. Note that the length of a data generator\n",
    "    # is defined as the number of batches required to produce a total of roughly 1000\n",
    "    # sequences given a batch size.\n",
    "    for batch_idx in range(len(train_data_gen)):\n",
    "\n",
    "        # Request a batch of sequences and class labels, convert them into tensors\n",
    "        # of the correct type, and then send them to the appropriate device.\n",
    "        data, target = train_data_gen[batch_idx]\n",
    "        data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n",
    "\n",
    "        # Perform the forward pass of the model\n",
    "        output = model(data)\n",
    "\n",
    "        # Pick only the output corresponding to last sequence element (input is pre padded)\n",
    "        output = output[:, -1, :]\n",
    "\n",
    "        # Compute the value of the loss for this batch. For loss functions like CrossEntropyLoss,\n",
    "        # the second argument is actually expected to be a tensor of class indices rather than\n",
    "        # one-hot encoded class labels. One approach is to take advantage of the one-hot encoding\n",
    "        # of the target and call argmax along its second dimension to create a tensor of shape\n",
    "        # (batch_size) containing the index of the class label that was hot for each sequence.\n",
    "        target = target.argmax(dim=1)\n",
    "\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # Clear the gradient buffers of the optimized parameters.\n",
    "        # Otherwise, gradients from the previous batch would be accumulated.\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # the backward pass over the loss and the model\n",
    "        loss.backward()\n",
    "\n",
    "        # the gradient step to update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "        y_pred = output.argmax(dim=1)\n",
    "        num_correct += (y_pred == target).sum().item()\n",
    "\n",
    "    return num_correct, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the Testing Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_data_gen, criterion, device):\n",
    "    # Set the model to evaluation mode. This will turn off layers that would\n",
    "    # otherwise behave differently during training, such as dropout.\n",
    "    model.eval()\n",
    "\n",
    "    # Store the number of sequences that were classified correctly\n",
    "    num_correct = 0\n",
    "\n",
    "    # A context manager is used to disable gradient calculations during inference\n",
    "    # to reduce memory usage, as we typically don't need the gradients at this point.\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(test_data_gen)):\n",
    "            data, target = test_data_gen[batch_idx]\n",
    "            data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n",
    "\n",
    "            output = model(data)\n",
    "            # Pick only the output corresponding to last sequence element (input is pre padded)\n",
    "            output = output[:, -1, :]\n",
    "\n",
    "            target = target.argmax(dim=1)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "            y_pred = output.argmax(dim=1)\n",
    "            num_correct += (y_pred == target).sum().item()\n",
    "\n",
    "    return num_correct, loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting it All Together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from plot_lib import set_default, plot_state, print_colourbar\n",
    "set_default()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=True):\n",
    "    # Automatically determine the device that PyTorch should use for computation\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # Track the value of the loss function and model accuracy across epochs\n",
    "    history_train = {'loss': [], 'acc': []}\n",
    "    history_test = {'loss': [], 'acc': []}\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # Run the training loop and calculate the accuracy.\n",
    "        # Remember that the length of a data generator is the number of batches,\n",
    "        # so we multiply it by the batch size to recover the total number of sequences.\n",
    "        num_correct, loss = train(model, train_data_gen, criterion, optimizer, device)\n",
    "        accuracy = float(num_correct) / (len(train_data_gen) * train_data_gen.batch_size) * 100\n",
    "        history_train['loss'].append(loss)\n",
    "        history_train['acc'].append(accuracy)\n",
    "\n",
    "        # Do the same for the testing loop\n",
    "        num_correct, loss = test(model, test_data_gen, criterion, device)\n",
    "        accuracy = float(num_correct) / (len(test_data_gen) * test_data_gen.batch_size) * 100\n",
    "        history_test['loss'].append(loss)\n",
    "        history_test['acc'].append(accuracy)\n",
    "\n",
    "        if verbose or epoch + 1 == max_epochs:\n",
    "            print(f'[Epoch {epoch + 1}/{max_epochs}]'\n",
    "                  f\" loss: {history_train['loss'][-1]:.4f}, acc: {history_train['acc'][-1]:2.2f}%\"\n",
    "                  f\" - test_loss: {history_test['loss'][-1]:.4f}, test_acc: {history_test['acc'][-1]:2.2f}%\")\n",
    "\n",
    "    # Generate diagnostic plots for the loss and accuracy\n",
    "    fig, axes = plt.subplots(ncols=2, figsize=(9, 4.5))\n",
    "    for ax, metric in zip(axes, ['loss', 'acc']):\n",
    "        ax.plot(history_train[metric])\n",
    "        ax.plot(history_test[metric])\n",
    "        ax.set_xlabel('epoch', fontsize=12)\n",
    "        ax.set_ylabel(metric, fontsize=12)\n",
    "        ax.legend(['Train', 'Test'], loc='best')\n",
    "    plt.show()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Elman RNNs for 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training and test data generators\n",
    "difficulty     = QRSU.DifficultyLevel.EASY\n",
    "batch_size     = 32\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup the RNN and training settings\n",
    "input_size  = train_data_gen.n_symbols\n",
    "hidden_size = 4\n",
    "output_size = train_data_gen.n_classes\n",
    "model       = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.CrossEntropyLoss()\n",
    "optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs  = 10\n",
    "\n",
    "# Train the model\n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTMs for 10 epochs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training and test data generators\n",
    "difficulty     = QRSU.DifficultyLevel.EASY\n",
    "batch_size     = 32\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup the RNN and training settings\n",
    "input_size  = train_data_gen.n_symbols\n",
    "hidden_size = 4\n",
    "output_size = train_data_gen.n_classes\n",
    "model       = SimpleLSTM(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.CrossEntropyLoss()\n",
    "optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs  = 10\n",
    "\n",
    "# Train the model\n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elman: Increasing epoch to 50\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training and test data generators\n",
    "difficulty     = QRSU.DifficultyLevel.EASY\n",
    "batch_size     = 4\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup the RNN and training settings\n",
    "input_size  = train_data_gen.n_symbols\n",
    "hidden_size = 32\n",
    "output_size = train_data_gen.n_classes\n",
    "model       = SimpleRNN(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.CrossEntropyLoss()\n",
    "optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs  = 50\n",
    "\n",
    "# Train the model\n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTMs: Increasing epoch to 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the training and test data generators\n",
    "difficulty     = QRSU.DifficultyLevel.EASY\n",
    "batch_size     = 32\n",
    "train_data_gen = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "test_data_gen  = QRSU.get_predefined_generator(difficulty, batch_size)\n",
    "\n",
    "# Setup the RNN and training settings\n",
    "input_size  = train_data_gen.n_symbols\n",
    "hidden_size = 4\n",
    "output_size = train_data_gen.n_classes\n",
    "model       = SimpleLSTM(input_size, hidden_size, output_size)\n",
    "criterion   = torch.nn.CrossEntropyLoss()\n",
    "optimizer   = torch.optim.RMSprop(model.parameters(), lr=0.001)\n",
    "max_epochs  = 30\n",
    "\n",
    "# Train the model\n",
    "model = train_and_test(model, train_data_gen, test_data_gen, criterion, optimizer, max_epochs, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import random\n",
    "\n",
    "def evaluate_model(model, difficulty, seed=9001, verbose=False):\n",
    "    # Define a dictionary that maps class indices to labels\n",
    "    class_idx_to_label = {0: 'Q', 1: 'R', 2: 'S', 3: 'U'}\n",
    "\n",
    "    # Create a new data generator\n",
    "    data_generator = QRSU.get_predefined_generator(difficulty, seed=seed)\n",
    "\n",
    "    # Track the number of times a class appears\n",
    "    count_classes = collections.Counter()\n",
    "\n",
    "    # Keep correctly classified and misclassified sequences, and their\n",
    "    # true and predicted class labels, for diagnostic information.\n",
    "    correct = []\n",
    "    incorrect = []\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx in range(len(data_generator)):\n",
    "            data, target = test_data_gen[batch_idx]\n",
    "            data, target = torch.from_numpy(data).float().to(device), torch.from_numpy(target).long().to(device)\n",
    "\n",
    "            data_decoded = data_generator.decode_x_batch(data.numpy())\n",
    "            target_decoded = data_generator.decode_y_batch(target.numpy())\n",
    "\n",
    "            output = model(data)\n",
    "            sequence_end = torch.tensor([len(sequence) for sequence in data_decoded]) - 1\n",
    "            output = output[torch.arange(data.shape[0]).long(), sequence_end, :]\n",
    "\n",
    "            target = target.argmax(dim=1)\n",
    "            y_pred = output.argmax(dim=1)\n",
    "            y_pred_decoded = [class_idx_to_label[y.item()] for y in y_pred]\n",
    "\n",
    "            count_classes.update(target_decoded)\n",
    "            for i, (truth, prediction) in enumerate(zip(target_decoded, y_pred_decoded)):\n",
    "                if truth == prediction:\n",
    "                    correct.append((data_decoded[i], truth, prediction))\n",
    "                else:\n",
    "                    incorrect.append((data_decoded[i], truth, prediction))\n",
    "\n",
    "    num_sequences = sum(count_classes.values())\n",
    "    accuracy = float(len(correct)) / num_sequences * 100\n",
    "    print(f'The accuracy of the model is measured to be {accuracy:.2f}%.\\n')\n",
    "\n",
    "    # Report the accuracy by class\n",
    "    for label in sorted(count_classes):\n",
    "        num_correct = sum(1 for _, truth, _ in correct if truth == label)\n",
    "        print(f'{label}: {num_correct} / {count_classes[label]} correct')\n",
    "\n",
    "    # Report some random sequences for examination\n",
    "    print('\\nHere are some example sequences:')\n",
    "    for i in range(10):\n",
    "        sequence, truth, prediction = correct[random.randrange(0, 10)]\n",
    "        print(f'{sequence} -> {truth} was labelled {prediction}')\n",
    "\n",
    "    # Report misclassified sequences for investigation\n",
    "    if incorrect and verbose:\n",
    "        print('\\nThe following sequences were misclassified:')\n",
    "        for sequence, truth, prediction in incorrect:\n",
    "            print(f'{sequence} -> {truth} was labelled {prediction}')\n",
    "    else:\n",
    "        print('\\nThere were no misclassified sequences.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model, difficulty)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
